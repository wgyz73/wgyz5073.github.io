<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
 
  <title>Homepage</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Guoyizhe Wei</name>
              </p>
		  <p>
		    I am a third-year PhD student at <a href="http://jhu.edu">Johns Hopkins University</a>, where I am fortunate to be advised by
		    Bloomberg Distinguished Professor <a href="https://cs.jhu.edu/~ayuille/"> Rama Chellappa</a>.
		  </p>
              <p>
                Before that I was an M.S. student at <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>,
		    where I worked under the guidance of Prof. 
		    <a href="https://scholar.google.com/citations?user=Xrh1OIUAAAAJ&hl=en">Xiu Li</a>.
             </p>
             <p>
		    My current research interest lies at the vision-language understanding.
              </p>
	      <p style="color: red; font-weight: bold;">
		    I'm looking for research intern positions for Spring and/or Summer 2026. Feel free to contact me if my background aligns well with you directions.
              </p>
              <p style="text-align:center">
		    <a href="mailto:wgyz5073@gmail.com">wgyz5073 [at] gmail [dot] com</a> /
		    <a href="https://scholar.google.com/citations?user=uitAHvoAAAAJ&hl=en&authuser=1">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/icon_wgyz.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/icon_wgyz.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
		<p>[2024.07] Our SCLIP paper is accepted by ECCV 2024. It's an elegant way to extract dense CLIP features without training! </p>
		<p>[2023.08] Come to JHU and start my PhD life! </p>
	        <p>[2023.01] One paper accepted by ICLR 2023!</p>
              <p>[2022.07] Come to MSRA NLC group for internship!</p>
              <p>[2022.07] One paper accepted by ECCV 2022!</p>
              <p> ... </p>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

        <tr onmouseout="sclip_stop()" onmouseover="sclip_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <br></br>
              <div class="one">
                <div class="two" id='sclip_image'><img style="width:100%" src='images/sclip_1.png'></div>
                <img style="width:100%" src='images/sclip_0.png'>
              </div>
              <script type="text/javascript">
                function sclip_start() {
                  document.getElementById('sclip_image').style.opacity = "1";
                }

                function sclip_stop() {
                  document.getElementById('sclip_image').style.opacity = "0";
                }
                sclip_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle> SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference </papertitle>
              <br>
              <strong>Feng Wang</strong>,
              <a href="https://meijieru.com/">Jieru Mei</a>,
              <a href="http://cs.jhu.edu/~ayuille/">Alan Yuille</a>
              <br>
              <em>ECCV 2024 </em>. | <a href="https://arxiv.org/pdf/2312.01597.pdf">arXiv</a>
		    | <a href="https://github.com/wangf3014/SCLIP">code</a>
              <br>
              We present a zero-shot semantic segmentation model called SCLIP (Segmentation-adapted CLIP model),
		which leverages our newly proposed correlative self-attention mechanism and allows training-free
		adaptation to semantic segmentation tasks with CLIP.
              <br>
            </td>
          </tr>

	<tr onmouseout="adventurer_stop()" onmouseover="adventurer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <br></br>
              <div class="one">
                <div class="two" id='adventurer_image'><img style="width:100%" src='images/adventurer_1.png'></div>
                <img style="width:100%" src='images/adventurer_0.png'>
              </div>
              <script type="text/javascript">
                function adventurer_start() {
                  document.getElementById('adventurer_image').style.opacity = "1";
                }

                function adventurer_stop() {
                  document.getElementById('adventurer_image').style.opacity = "0";
                }
                adventurer_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle> Causal Image Modeling for Efficient Visual Understanding </papertitle>
              <br>
              <strong>Feng Wang</strong>,
	      Timing Yang,
	      <a href="https://yaodongyu.github.io/"> Yaodong Yu</a>,
	      <a href="https://oliverrensu.github.io/"> Sucheng Ren</a>,
	      Guoyizhe Wei,
	      <a href="https://angtianwang.github.io/"> Angtian Wang</a>,
	      <a href="https://nephrology.medicine.ufl.edu/profile/shao-wei-1/"> Wei Shao</a>,
	      <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
              <a href="http://cs.jhu.edu/~ayuille/">Alan Yuille</a>,
	      <a href="https://cihangxie.github.io/">Cihang Xie</a>,
              <br>
              <em>Preprint, under review </em>. | <a href="https://github.com/wangf3014/Adventurer">arXiv</a>
		    | <a href="https://github.com/wangf3014/Adventurer">code</a>
              <br>
              we present a comprehensive analysis of causal image modeling and introduce the 
		    Adventurer series models where we treat images as sequences of patch tokens 
		    and employ uni-directional language models to learn visual representations.
		    This modeling paradigm allows us to process images in a recurrent formulation with
		    linear complexity relative to the sequence length, which can effectively address
		    the memory and computation explosion issues posed by high-resolution and fine-grained images.
              <br>
            </td>
          </tr>

	<tr onmouseout="mambar_stop()" onmouseover="mambar_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <br></br>
              <div class="one">
                <div class="two" id='mambar_image'><img style="width:100%" src='images/mambar_1.png'></div>
                <img style="width:100%" src='images/mambar_0.png'>
              </div>
              <script type="text/javascript">
                function mambar_start() {
                  document.getElementById('mambar_image').style.opacity = "1";
                }

                function mambar_stop() {
                  document.getElementById('mambar_image').style.opacity = "0";
                }
                mambar_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle> Mamba-Reg: Vision Mamba Also Needs Registers </papertitle>
              <br>
              <strong>Feng Wang</strong>,
              <a href="https://jiahaoplus.github.io/"> Jiahao Wang</a>,
	      <a href="https://oliverrensu.github.io/"> Sucheng Ren</a>,
	      Guoyizhe Wei,
	      <a href="https://meijieru.com/">Jieru Mei</a>,
	      <a href="https://nephrology.medicine.ufl.edu/profile/shao-wei-1/"> Wei Shao</a>,
	      <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
              <a href="http://cs.jhu.edu/~ayuille/">Alan Yuille</a>,
	      <a href="https://cihangxie.github.io/">Cihang Xie</a>,
              <br>
              <em>Preprint, under review </em>. | <a href="https://arxiv.org/pdf/2405.14858.pdf">arXiv</a>
		    | <a href="https://github.com/wangf3014/Mamba-Reg">code</a>
              <br>
              Similar to Vision Transformers, we identify artifacts also present within the feature maps of Vision Mamba.
	These artifacts, corresponding to high-norm tokens emerging in low-information background areas of images, appear much more severe in Vision Mamba.
              <br>
            </td>
          </tr>
	  
	<tr onmouseout="cp2_stop()" onmouseover="cp2_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <br></br>
              <div class="one">
                <div class="two" id='cp2_image'><img style="width:100%" src='images/cp2_1.png'></div>
                <img style="width:100%" src='images/cp2_0.png'>
              </div>
              <script type="text/javascript">
                function cp2_start() {
                  document.getElementById('cp2_image').style.opacity = "1";
                }

                function cp2_stop() {
                  document.getElementById('cp2_image').style.opacity = "0";
                }
                cp2_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle> CP2: Copy-Paste Contrastive Pretraining for Semantic Segmentation </papertitle>
              <br>
              <strong>Feng Wang</strong>,
              <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
              <a href="https://weichen582.github.io/">Chen Wei</a>,
              <a href="http://cs.jhu.edu/~ayuille/">Alan Yuille</a>,
              <a href="https://shenwei1231.github.io/">Wei Shen</a>
              <br>
              <em>ECCV </em>, 2022 | 
              <a href="https://arxiv.org/pdf/2203.11709">arXiv</a>  /
              <a href="https://github.com/wangf3014/CP2">code</a>
              <br>
              We propose a dense (pixel-wise) self-supervised contrastive learning method called CP2,
              which facilitates both image- and pixel-level representations.
              We obtain 78.6% mIoU with a ResNet-50 and 79.5% with a ViT-S by finetuning CP2 pretrained models on PASCAL VOC.
            </td>
          </tr>

          <tr onmouseout="defo_stop()" onmouseover="defo_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <br></br>
              <div class="one">
                <div class="two" id='defo_image'><img style="width:100%" src='images/defo_1.png'></div>
                <img style="width:100%" src='images/defo_0.png'>
              </div>
              <script type="text/javascript">
                function defo_start() {
                  document.getElementById('defo_image').style.opacity = "1";
                }

                function defo_stop() {
                  document.getElementById('defo_image').style.opacity = "0";
                }
                defo_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle> Learning to Decompose Visual Features with Latent Textual Prompts </papertitle>
              <br>
              <strong>Feng Wang</strong>,
              <a href="https://limanling.github.io/">Manling Li</a>,
              <a href="https://xudonglinthu.github.io/">Xudong Lin</a>,
	      <a href="https://scholar.google.com/citations?user=WU1tm2EAAAAJ&hl=zh-CN&oi=ao">Hairong Lv</a>,
	      <a href="https://alexander-schwing.de/">Alexander G. Schwing</a>,
              <a href="http://blender.cs.illinois.edu/hengji.html">Heng Ji</a>
              <br>
              <em>ICLR </em>, 2023. | <a href="https://arxiv.org/pdf/2210.04287">arXiv</a>
              <br>
              We propose a novel vision-language model called Decomposed Feature Prompting (short as DeFo),
              which decouples the language inputs from the classes to be inferred,
              and learns to extract detailed visual features with textual prompts.
              <br>
            </td>
          </tr>

          

          <tr onmouseout="dpt_stop()" onmouseover="dpt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <br></br>
              <div class="one">
                <div class="two" id='dpt_image'><img style="width:100%" src='images/dpt_1.png'></div>
                <img style="width:100%" src='images/dpt_0.png'>
              </div>
              <script type="text/javascript">
                function dpt_start() {
                  document.getElementById('dpt_image').style.opacity = "1";
                }

                function dpt_stop() {
                  document.getElementById('dpt_image').style.opacity = "0";
                }
                dpt_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle> Dual Prompt Tuning for Domain-Aware Federated Learning </papertitle>
              <br>
		Guoyizhe Wei, <strong>Feng Wang</strong>, Anshul Shah, Rama Chellappa
              <br>
              <em>Preprint, under review </em>. | <a href="https://arxiv.org/pdf/2310.03103.pdf">arXiv</a>
              <br>
              We address the challenges of domain shift in vision-language inference by leveraging
		the technique of prompt learning for both the image and text encoders in CLIP,
		which facilitates domain adaptation over decentralized and non-iid data.
              <br>
            </td>
          </tr>


          <tr onmouseout="cbnn_stop()" onmouseover="cbnn_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <br></br>
              <div class="one">
                <div class="two" id='cbnn_image'><img style="width:100%" src='images/cbnn_1.png'></div>
                <img style="width:100%" src='images/cbnn_0.png'>
              </div>
              <script type="text/javascript">
                function cbnn_start() {
                  document.getElementById('cbnn_image').style.opacity = "1";
                }

                function cbnn_stop() {
                  document.getElementById('cbnn_image').style.opacity = "0";
                }
                cbnn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle> Boost Neural Networks by Checkpoints </papertitle>
              <br>
              <strong>Feng Wang</strong>,
              Guoyizhe Wei,
              <a href="http://liuqiao.me/">Qiao Liu</a>,
              Jinxiang Ou,
              <a href="https://scholar.google.com/citations?user=TaEM1KcAAAAJ&hl=zh-CN&oi=ao">Xian Wei</a>,
              <a href="https://scholar.google.com/citations?user=WU1tm2EAAAAJ&hl=zh-CN&oi=ao">Hairong Lv</a>
              <br>
              <em>NeurIPS </em>, 2021 | 
              <a href="https://arxiv.org/pdf/2110.00959.pdf">arXiv</a>
              <br>
              We propose a novel checkpoint ensemble called Checkpoint Boosted Neural Networks (CBNN),
              where a boosting scheme is utilized to accelerate model convergence and maximize the checkpoint diversity.
              Our superior performance is supported by a theoretical proof. 
              <br>
            </td>
          </tr>

          <tr onmouseout="gbf_stop()" onmouseover="gbf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <br></br>
              <div class="one">
                <div class="two" id='gbf_image'><img style="width:100%" src='images/gbf.png'></div>
                <img style="width:100%" src='images/gbf.png'>
              </div>
              <script type="text/javascript">
                function gbf_start() {
                  document.getElementById('gbf_image').style.opacity = "1";
                }

                function gbf_stop() {
                  document.getElementById('gbf_image').style.opacity = "0";
                }
                gbf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle> Gradient Boosting Forest: a Two-Stage Ensemble Method Enabling Federated Learning of GBDTs </papertitle>
              <br>
              <strong>Feng Wang</strong>,
              Jinxiang Ou,
              <a href="https://scholar.google.com/citations?user=WU1tm2EAAAAJ&hl=zh-CN&oi=ao">Hairong Lv</a>
              <br>
              <em>ICONIP </em>, 2021 | 
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-92270-2_7">paper</a>
              <br>
              We propose a novel GBDT model which extends each single decision tree of GBDT to an ensemble of trees that are
              trained from different  data splits. Our method allows decentralized training and achieves more robust performance.
            </td>
          </tr>

        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                  Last update: Dec. 2023 &nbsp&nbsp&nbsp&nbsp <a href="https://jonbarron.info/">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
